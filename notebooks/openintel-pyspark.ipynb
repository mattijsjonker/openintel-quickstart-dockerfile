{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenINTEL data quickstart notebook<br/>\n",
    "Author: Mattijs Jonker <m.jonker@utwente.nl><br/>\n",
    "Copyright -- all rights reserved -- 2019<br/>\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook and the Dockerfile that comes with is meant to serve as a quickstart towards exploring and analyzing (public) OpenINTEL data. In essence, we will build a custom docker container that comes with Jupyter, Spark and Hadoop installed. The container will run a Jupyter notebook server to run the notebook you are currently reading. Within this notebook we will create a Spark application, load some OpenINTEL measurement data (Avro), and perform a few basic analyses to get you on your way.\n",
    "\n",
    "Please note that this notebook essentially starts a local Spark application that reads data from a local Hadoop filesystem. If you have a Hadoop cluster in place and use, for example, Spark on YARN and perhaps even Kerberos authentication, your docker container needs additional packages and configuration files, and your notebook needs additional configuration directives. Within the OpenINTEL project we have a Dockerfile specific to our setup. Feel free to reach out to us should you need help setting up something similar for your own environment.\n",
    "\n",
    "**Please perform the below steps to get started**\n",
    "\n",
    "(for now, these instructions are for unix-based systems only)\n",
    "\n",
    "### Step 1: Build OpenINTEL quickstart docker image\n",
    "\n",
    "1. We will assume that you already have a docker daemon up and running on the host machine. In case you do not, instructions for installation can easily be found online.\n",
    "2. Clone the git repository with the OpenINTEL quickstart docker files: *git clone https://github.com/mattijsjonker/openintel-quickstart-dockerfile*\n",
    "3. Build the OpenINTEL quickstart docker image: *cd openintel-quickstart-dockerfile && sudo docker build -t openintel-quickstart .* (alternatively, use *sudo ./build-docker.sh*)\n",
    "    1. Monitor the build output. A successful build is indicated with: *Successfully built <ID>*.\n",
    "    2. Optionally, you can verify that the image has been built with the command: *sudo docker images*. The output should show you an *openintel-quickstart* image of about 2.3GiB\n",
    "\n",
    "### Step 2: Download OpenINTEL data\n",
    "    \n",
    "1. Download a sample of the (public) OpenINTEL data and place it in the *data* folder. You can do this placement on the host machine as the directory will be shared with our docker container\n",
    "    1. cd data && mkdir -p source=alexa/year=2019/month=09/day=01 && cd $_\n",
    "    2. wget https://data.openintel.nl/data/alexa1m/2019/openintel-alexa1m-20190101.tar && tar -xvf openintel-alexa1m-20190101.tar && rm openintel-alexa1m-20190101.tar\n",
    "    3. Repeat this for a few more days if you wish (evidently, more data means longer example execution time)\n",
    "    \n",
    "### Step 3: Run a container using the newly built image\n",
    "    \n",
    "1. Run a docker container using the image we just built\n",
    "    1. You can run a container with the command: *sudo ./run-interactive-docker.sh*\n",
    "    2. This will create a docker container with the name *openintel-quickstart*\n",
    "    3. This will \"expose\" the 8888/TCP port for the notebook server running inside the container. Please be aware that on Linux systems, the port will be world-accessible unless you have explicitly taken steps to prevent this. More information can be found [here](https://docs.docker.com/network/iptables/).\n",
    "    \n",
    "The parameters in *run-interactive-docker.sh* will run the container in attached mode. You can detach with ^P^Q and reattach with *sudo docker attach openintel-quickstart*. You can also stop and start the container as you please. These topics are out of scope. Please refer to the Docker [documentation](https://docs.docker.com/get-started/) for more information.\n",
    "\n",
    "### Step 4: Access the Jupyter server running inside the container\n",
    "\n",
    "1. You can access Jupyter Notebook at http://localhost:8888/lab\n",
    "2. The password is 'openintel'\n",
    "3. This quickstart notebook can be found under */home/openintel/notebooks* (you may have to open the filebrowser with CTRL+B)\n",
    "\n",
    "### Step 5: run example code\n",
    "\n",
    "1. You can run the example code below. Please make sure to run the *Imports*, *Configuration* and *Start SparkContext* code blocks before running any of the example code.\n",
    "    1. Note that each analytic example comes in two separate forms: one form uses DataFrame abstractions and the other uses Spark SQL. The results for each pair of examples are the same. This allows you to pick the approach you are more comfortable or familiar with\n",
    "    2. You can follow the Spark application's progress in the Jupyter Notebook output, provided you are attached to the running docker container\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta, date\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import IPython.display\n",
    "import re\n",
    "import pyarrow as pa\n",
    "\n",
    "# Find Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports\n",
    "import pyspark\n",
    "import pyspark.sql.functions as psf\n",
    "import pyspark.sql.types as pst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file:/home/openintel/data/source=alexa']\n"
     ]
    }
   ],
   "source": [
    "# Create a (local) HDFS connection\n",
    "# n.b.: not strictly required for this quickstart notebook\n",
    "hdfs_fs = pa.hdfs.connect(host=u\"default\", port=0, user=u\"openintel\", driver=u\"libhdfs\")\n",
    "\n",
    "# Set the OpenINTEL data directory\n",
    "DATA_DIRECTORY = \"/home/openintel/data\"\n",
    "\n",
    "# The list of sources, i.e., TLDs, to investigate\n",
    "SOURCES = [\"alexa\"]\n",
    "\n",
    "print(hdfs_fs.ls(DATA_DIRECTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkConf\n",
    "APP_NAME = \"openintel-quickstart\"\n",
    "spark_conf = pyspark.SparkConf().setAppName(APP_NAME).setMaster(\"local\"\n",
    ").set(\"spark.driver.cores\",\"2\"\n",
    ").set(\"spark.driver.memory\",\"2G\"\n",
    "# Note: You may have to fiddle with the executor cores, memory and memoryOverhead to fit your docker container's available resources\n",
    "#       For quickstart purposes, the default settings suffice.\n",
    ").set(\"spark.executor.cores\", \"4\"\n",
    ").set(\"spark.executor.memory\", \"6G\").set(\"spark.executor.memoryOverhead\", \"2G\"\n",
    ").set(\"spark.dynamicAllocation.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext\n",
    "sc = pyspark.SparkContext(conf=spark_conf)\n",
    "\n",
    "# SQLContext\n",
    "sqlc = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented to prevent accidental sequential execution\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code (verbatim SQL)\n",
    "Here we show how to work with Spark SQL. This involves loading a dataframe, registering a view, and then performing queries using that view.\n",
    "1. Load Spark DataFrame\n",
    "2. Register view / temp table\n",
    "3. Perform Spark SQL queries\n",
    "4. Drop temp table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Spark DF and register view *mdata*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query_type', 'query_name', 'response_type', 'response_name', 'response_ttl', 'timestamp', 'rtt', 'worker_id', 'status_code', 'ip4_address', 'ip6_address', 'country', 'as', 'as_full', 'ip_prefix', 'cname_name', 'dname_name', 'mx_address', 'mx_preference', 'mxset_hash_algorithm', 'mxset_hash', 'ns_address', 'nsset_hash_algorithm', 'nsset_hash', 'txt_text', 'txt_hash_algorithm', 'txt_hash', 'ds_key_tag', 'ds_algorithm', 'ds_digest_type', 'ds_digest', 'dnskey_flags', 'dnskey_protocol', 'dnskey_algorithm', 'dnskey_pk_rsa_n', 'dnskey_pk_rsa_e', 'dnskey_pk_rsa_bitsize', 'dnskey_pk_eccgost_x', 'dnskey_pk_eccgost_y', 'dnskey_pk_dsa_t', 'dnskey_pk_dsa_q', 'dnskey_pk_dsa_p', 'dnskey_pk_dsa_g', 'dnskey_pk_dsa_y', 'dnskey_pk_eddsa_a', 'dnskey_pk_wire', 'nsec_next_domain_name', 'nsec_owner_rrset_types', 'nsec3_hash_algorithm', 'nsec3_flags', 'nsec3_iterations', 'nsec3_salt', 'nsec3_next_domain_name_hash', 'nsec3_owner_rrset_types', 'nsec3param_hash_algorithm', 'nsec3param_flags', 'nsec3param_iterations', 'nsec3param_salt', 'spf_text', 'spf_hash_algorithm', 'spf_hash', 'soa_mname', 'soa_rname', 'soa_serial', 'soa_refresh', 'soa_retry', 'soa_expire', 'soa_minimum', 'rrsig_type_covered', 'rrsig_algorithm', 'rrsig_labels', 'rrsig_original_ttl', 'rrsig_signature_inception', 'rrsig_signature_expiration', 'rrsig_key_tag', 'rrsig_signer_name', 'rrsig_signature', 'cds_key_tag', 'cds_algorithm', 'cds_digest_type', 'cds_digest', 'cdnskey_flags', 'cdnskey_protocol', 'cdnskey_algorithm', 'cdnskey_pk_rsa_n', 'cdnskey_pk_rsa_e', 'cdnskey_pk_rsa_bitsize', 'cdnskey_pk_eccgost_x', 'cdnskey_pk_eccgost_y', 'cdnskey_pk_dsa_t', 'cdnskey_pk_dsa_q', 'cdnskey_pk_dsa_p', 'cdnskey_pk_dsa_g', 'cdnskey_pk_dsa_y', 'cdnskey_pk_eddsa_a', 'cdnskey_pk_wire', 'caa_flags', 'caa_tag', 'caa_value', 'tlsa_usage', 'tlsa_selector', 'tlsa_matchtype', 'tlsa_certdata', 'ptr_name', 'source', 'year', 'month', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Create Spark DF\n",
    "spark_df = sqlc.read.option(\"basePath\", \"/\").format(\"avro\").load(*[\n",
    "    # n.b.: set paths list as desired: /home/openintel/data/source=<tld>/year=<yyyy>/month=<mm>/day=<dd>\n",
    "    os.path.join(DATA_DIRECTORY, \"source={}\".format(i_source)) for i_source in SOURCES\n",
    "])\n",
    "\n",
    "# Note that you may have to explicitly specify the Avro schema in case the Avro files you load are \"evolved\" w.r.t. each other. This can\n",
    "# technically happen with the public OpenINTEL data, but only if your analysis involves older data. You can either extract the schema from\n",
    "# a newer file and supply it as read option like so: *.option(\"avroSchema\", ...)* or reach out to us for the latest .avsc.\n",
    "\n",
    "# Register temporary table\n",
    "tt_name = \"mdata\"\n",
    "spark_df.createOrReplaceTempView(tt_name)\n",
    "\n",
    "# Show the colums to work with\n",
    "# see also: https://openintel.nl/background/dictionary/\n",
    "print(spark_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Perform some example Spark SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n",
      "|source|date      |record_count|\n",
      "+------+----------+------------+\n",
      "|alexa |2019-09-01|5022012     |\n",
      "|alexa |2019-09-02|2677058     |\n",
      "|alexa |2019-09-03|2683449     |\n",
      "|alexa |2019-09-04|2872323     |\n",
      "+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Number of records per (source, day)\n",
    "# We simply count the number of measurement data records per (source, day)\n",
    "\n",
    "# Verbatim Spark SQL query\n",
    "spark_sql_query_string = \"\"\"\n",
    "SELECT source,\n",
    "       CONCAT_WS(\"-\", year, LPAD(month, 2, \"0\"), LPAD(day, 2, \"0\")) AS date, \n",
    "       COUNT(1) AS record_count\n",
    "FROM {} \n",
    "WHERE year = 2019 AND month = 9 AND day <= 7\n",
    "GROUP BY source, date\n",
    "ORDER BY source, date\n",
    "\"\"\".format(tt_name)\n",
    "\n",
    "spark_sql_query_r = sqlc.sql(spark_sql_query_string)\n",
    "spark_sql_query_r.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+\n",
      "|source|date      |ip4_address_count|\n",
      "+------+----------+-----------------+\n",
      "|alexa |2019-09-01|262105           |\n",
      "|alexa |2019-09-02|161305           |\n",
      "|alexa |2019-09-03|157303           |\n",
      "+------+----------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Number of distinct A records per (source, day)\n",
    "# We count the number of distinct IPv4 addresses per (source, day)\n",
    "\n",
    "# Verbatim Spark SQL query\n",
    "spark_sql_query_string = \"\"\"\n",
    "SELECT source,\n",
    "       CONCAT_WS(\"-\", year, LPAD(month, 2, \"0\"), LPAD(day, 2, \"0\")) AS date,\n",
    "       COUNT(DISTINCT(ip4_address)) AS ip4_address_count\n",
    "FROM {} \n",
    "WHERE year = 2019 AND month = 9 AND day <= 7\n",
    "GROUP BY source, date\n",
    "ORDER BY source, date\n",
    "\"\"\".format(tt_name)\n",
    "\n",
    "spark_sql_query_r = sqlc.sql(spark_sql_query_string)\n",
    "spark_sql_query_r.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------+\n",
      "|source|date      |qname_count|\n",
      "+------+----------+-----------+\n",
      "|alexa |2019-09-01|39506      |\n",
      "+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Number of distinct domain names with a CloudFlare name server\n",
    "# We count the number of distinct domain names per (source, day),\n",
    "# provided the domain name has an NS resource record that ends in .ns.cloudflare.com.\n",
    "\n",
    "# Verbatim Spark SQL query\n",
    "spark_sql_query_string = \"\"\"\n",
    "SELECT source,\n",
    "       CONCAT_WS(\"-\", year, LPAD(month, 2, \"0\"), LPAD(day, 2, \"0\")) AS date, \n",
    "       COUNT(DISTINCT(query_name)) AS qname_count\n",
    "FROM {} \n",
    "WHERE year = 2019 AND month = 9 AND day = 1\n",
    "      AND ns_address LIKE '%.ns.cloudflare.com.'\n",
    "GROUP BY source, date\n",
    "ORDER BY source, date\n",
    "\"\"\".format(tt_name)\n",
    "\n",
    "spark_sql_query_r = sqlc.sql(spark_sql_query_string)\n",
    "spark_sql_query_r.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|source|date      |cf_ns_name|\n",
      "+------+----------+----------+\n",
      "|alexa |2019-09-01|abby      |\n",
      "|alexa |2019-09-01|ada       |\n",
      "|alexa |2019-09-01|adam      |\n",
      "+------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Distinct CloudFlare name servers names\n",
    "# We get the distinct CloudFlare NS record names on 2019-09-01\n",
    "\n",
    "# Verbatim Spark SQL query\n",
    "spark_sql_query_string = \"\"\"\n",
    "SELECT DISTINCT source,\n",
    "       CONCAT_WS(\"-\", year, LPAD(month, 2, \"0\"), LPAD(day, 2, \"0\")) AS date, \n",
    "       REGEXP_EXTRACT(ns_address, '([^.]+)[.]ns[.]cloudflare.com[.]$', 1) AS cf_ns_name\n",
    "FROM {} \n",
    "WHERE year = 2019 AND month = 9 AND day = 1\n",
    "      AND ns_address LIKE '%.ns.cloudflare.com.'\n",
    "ORDER BY cf_ns_name\n",
    "\"\"\".format(tt_name)\n",
    "\n",
    "spark_sql_query_r = sqlc.sql(spark_sql_query_string)\n",
    "spark_sql_query_r.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Drop the temporary view *mdata*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Drop the temporary table\n",
    "sqlc.dropTempTable(tt_name)\n",
    "print(sqlc.tableNames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code (DF abstractions)\n",
    "Here we show how to work with DF abstractions to accomplish the same as in the above SQL queries. This involves loading a dataframe and then performing operations on the DF\n",
    "1. Load Spark DataFrame\n",
    "2. Perform Spark DF operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load a Spark DataFrame\n",
    "- Note that, differently from above, we add an ISO8601 date column to the DF for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query_type', 'query_name', 'response_type', 'response_name', 'response_ttl', 'timestamp', 'rtt', 'worker_id', 'status_code', 'ip4_address', 'ip6_address', 'country', 'as', 'as_full', 'ip_prefix', 'cname_name', 'dname_name', 'mx_address', 'mx_preference', 'mxset_hash_algorithm', 'mxset_hash', 'ns_address', 'nsset_hash_algorithm', 'nsset_hash', 'txt_text', 'txt_hash_algorithm', 'txt_hash', 'ds_key_tag', 'ds_algorithm', 'ds_digest_type', 'ds_digest', 'dnskey_flags', 'dnskey_protocol', 'dnskey_algorithm', 'dnskey_pk_rsa_n', 'dnskey_pk_rsa_e', 'dnskey_pk_rsa_bitsize', 'dnskey_pk_eccgost_x', 'dnskey_pk_eccgost_y', 'dnskey_pk_dsa_t', 'dnskey_pk_dsa_q', 'dnskey_pk_dsa_p', 'dnskey_pk_dsa_g', 'dnskey_pk_dsa_y', 'dnskey_pk_eddsa_a', 'dnskey_pk_wire', 'nsec_next_domain_name', 'nsec_owner_rrset_types', 'nsec3_hash_algorithm', 'nsec3_flags', 'nsec3_iterations', 'nsec3_salt', 'nsec3_next_domain_name_hash', 'nsec3_owner_rrset_types', 'nsec3param_hash_algorithm', 'nsec3param_flags', 'nsec3param_iterations', 'nsec3param_salt', 'spf_text', 'spf_hash_algorithm', 'spf_hash', 'soa_mname', 'soa_rname', 'soa_serial', 'soa_refresh', 'soa_retry', 'soa_expire', 'soa_minimum', 'rrsig_type_covered', 'rrsig_algorithm', 'rrsig_labels', 'rrsig_original_ttl', 'rrsig_signature_inception', 'rrsig_signature_expiration', 'rrsig_key_tag', 'rrsig_signer_name', 'rrsig_signature', 'cds_key_tag', 'cds_algorithm', 'cds_digest_type', 'cds_digest', 'cdnskey_flags', 'cdnskey_protocol', 'cdnskey_algorithm', 'cdnskey_pk_rsa_n', 'cdnskey_pk_rsa_e', 'cdnskey_pk_rsa_bitsize', 'cdnskey_pk_eccgost_x', 'cdnskey_pk_eccgost_y', 'cdnskey_pk_dsa_t', 'cdnskey_pk_dsa_q', 'cdnskey_pk_dsa_p', 'cdnskey_pk_dsa_g', 'cdnskey_pk_dsa_y', 'cdnskey_pk_eddsa_a', 'cdnskey_pk_wire', 'caa_flags', 'caa_tag', 'caa_value', 'tlsa_usage', 'tlsa_selector', 'tlsa_matchtype', 'tlsa_certdata', 'ptr_name', 'source', 'year', 'month', 'day', 'date']\n"
     ]
    }
   ],
   "source": [
    "# Create Spark DF\n",
    "spark_df = sqlc.read.option(\"basePath\", \"/\").format(\"avro\").load(*[\n",
    "    # n.b.: set paths list as desired: /home/openintel/data/source=<tld>/year=<yyyy>/month=<mm>/day=<dd>\n",
    "    os.path.join(DATA_DIRECTORY, \"source={}\".format(i_source)) for i_source in SOURCES\n",
    "]).withColumn(\n",
    "    # Add an ISO8601 date column\n",
    "    # n.b.: read() will produce integers for the year, month and day partitions in the HDFS file hierarchy\n",
    "    \"date\", psf.concat_ws('-', \"year\", psf.lpad(\"month\", 2, '0'), psf.lpad(\"day\", 2, '0')))\n",
    "\n",
    "# Note that you may have to explicitly specify the Avro schema in case the Avro files you load are \"evolved\" w.r.t. each other. This can\n",
    "# technically happen with the public OpenINTEL data, but only if your analysis involves older data. You can either extract the schema from\n",
    "# a newer file and supply it as read option like so: *.option(\"avroSchema\", ...)* or reach out to us for the latest .avsc.\n",
    "\n",
    "# Show the colums to work with\n",
    "# see also: https://openintel.nl/background/dictionary/\n",
    "print(spark_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n",
      "|source|date      |record_count|\n",
      "+------+----------+------------+\n",
      "|alexa |2019-09-01|5022012     |\n",
      "|alexa |2019-09-02|2677058     |\n",
      "|alexa |2019-09-03|2683449     |\n",
      "|alexa |2019-09-04|2872323     |\n",
      "+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Number of records per (source, day)\n",
    "# We simply count the number of measurement data records per (source, day)\n",
    "spark_df.filter(\n",
    "    (psf.col(\"year\") == 2019) & (psf.col(\"month\") == 9) & (psf.col(\"day\") <= 7)\n",
    ").select(\n",
    "    [\"source\", \"date\"]\n",
    ").groupby(\n",
    "    [\"source\", \"date\"]\n",
    ").agg(\n",
    "    psf.count(psf.lit(\"1\")).alias(\"record_count\")\n",
    ").orderBy(\"date\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+\n",
      "|source|date      |ip4_address_count|\n",
      "+------+----------+-----------------+\n",
      "|alexa |2019-09-01|262105           |\n",
      "|alexa |2019-09-02|161305           |\n",
      "|alexa |2019-09-03|157303           |\n",
      "|alexa |2019-09-04|191823           |\n",
      "+------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Number of distinct A records per (source, day)\n",
    "# We count the number of distinct IPv4 addresses per (source, day)\n",
    "spark_df.filter(\n",
    "    (psf.col(\"year\") == 2019) & (psf.col(\"month\") == 9) & (psf.col(\"day\") <= 7)\n",
    ").select(\n",
    "    [\"source\", \"date\", \"ip4_address\"]\n",
    ").groupby(\n",
    "    [\"source\", \"date\"]\n",
    ").agg(\n",
    "    psf.countDistinct(psf.col(\"ip4_address\")).alias(\"ip4_address_count\")\n",
    ").orderBy(\"date\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------+\n",
      "|source|date      |qname_count|\n",
      "+------+----------+-----------+\n",
      "|alexa |2019-09-01|39506      |\n",
      "+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Number of distinct domain names with a CloudFlare name server\n",
    "# We count the number of distinct domain names per (source, day),\n",
    "# provided the domain name has an NS resource record that ends in .ns.cloudflare.com.\n",
    "spark_df.filter(\n",
    "    (psf.col(\"year\") == 2019) & (psf.col(\"month\") == 9) & (psf.col(\"day\") == 1) &\n",
    "    (psf.col(\"ns_address\").endswith(\".ns.cloudflare.com.\"))\n",
    ").select(\n",
    "    [\"source\", \"date\", \"query_name\"]\n",
    ").groupby(\n",
    "    [\"source\", \"date\"]\n",
    ").agg(\n",
    "    psf.countDistinct(psf.col(\"query_name\")).alias(\"qname_count\")\n",
    ").orderBy(\"date\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|source|date      |cf_ns_name|\n",
      "+------+----------+----------+\n",
      "|alexa |2019-09-01|abby      |\n",
      "|alexa |2019-09-01|ada       |\n",
      "|alexa |2019-09-01|adam      |\n",
      "+------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Distinct CloudFlare name servers names\n",
    "# We get the distinct CloudFlare NS record names on 2019-09-01\n",
    "spark_df.filter(\n",
    "    (psf.col(\"year\") == 2019) & (psf.col(\"month\") == 9) & (psf.col(\"day\") == 1) &\n",
    "    (psf.col(\"ns_address\").endswith(\".ns.cloudflare.com.\"))\n",
    ").select(\n",
    "    [\"source\", \"date\", psf.regexp_extract(psf.col(\"ns_address\"), \"([^.]+)[.]ns[.]cloudflare.com[.]$\", 1).alias(\"cf_ns_name\")]\n",
    ").distinct(\n",
    ").orderBy(\"cf_ns_name\"\n",
    ").show(3, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
